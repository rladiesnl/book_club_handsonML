{
  "hash": "74e1f3fc3177069e28fb4da94a57a76f",
  "result": {
    "markdown": "---\ntitle: \"Chp 14 Support Vector Machines\"\nsubtitle: \"Hands-on Machine Learning with R<BR>Boookclub R-Ladies Utrecht and R-Ladies Den Bosch\"\nauthor: \"[Martine Jansen](https://fosstodon.org/@MartineJansen)\"\nformat: \n  rladies-revealjs:\n    slide-number: true\n    pdf-max-pages-per-slide: 1\n    footer: \"[RLadies Utrecht & RLadies Den Bosch, Boookclub HOMLR](https://github.com/rladiesnl/book_club_handsonML)\"\n    incremental: true\nself-contained: true\n---\n\n::: {.cell}\n\n:::\n\n\n## stRt {.smaller}\n\n-   Organized by [\\@RLadiesUtrecht](https://twitter.com/RLadiesUtrecht) and [\\@RLadiesDenBosch](https://twitter.com/RLadiesDenBosch)\n-   Meet-ups every 2 weeks on [\"Hands-On Machine Learning with R\"](https://bradleyboehmke.github.io/HOML/)\\\n    by Bradley Boehmke and Brandon Greenwell\n-   No session recording!But we will publish the slides and notes!\n-   We use HackMD for making shared notes and for the registry:<BR> <https://hackmd.io/rGu7xw2bRS-lm8lq7-wvXw>\n-   Please keep mic off during presentation. Nice to have camera on and participate to make the meeting more interactive.\n-   Questions? Raise hand / write question in HackMD or in chat\n-   Remember presenters are not necessarily also subject experts `<svg aria-hidden=\"true\" role=\"img\" viewBox=\"0 0 512 512\" style=\"height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:purple;overflow:visible;position:relative;\"><path d=\"M256 352C293.2 352 319.2 334.5 334.4 318.1C343.3 308.4 358.5 307.7 368.3 316.7C378 325.7 378.6 340.9 369.6 350.6C347.7 374.5 309.7 400 256 400C202.3 400 164.3 374.5 142.4 350.6C133.4 340.9 133.1 325.7 143.7 316.7C153.5 307.7 168.7 308.4 177.6 318.1C192.8 334.5 218.8 352 256 352zM208.4 208C208.4 225.7 194 240 176.4 240C158.7 240 144.4 225.7 144.4 208C144.4 190.3 158.7 176 176.4 176C194 176 208.4 190.3 208.4 208zM304.4 208C304.4 190.3 318.7 176 336.4 176C354 176 368.4 190.3 368.4 208C368.4 225.7 354 240 336.4 240C318.7 240 304.4 225.7 304.4 208zM512 256C512 397.4 397.4 512 256 512C114.6 512 0 397.4 0 256C0 114.6 114.6 0 256 0C397.4 0 512 114.6 512 256zM256 48C141.1 48 48 141.1 48 256C48 370.9 141.1 464 256 464C370.9 464 464 370.9 464 256C464 141.1 370.9 48 256 48z\"/></svg>`{=html}\n-   Remember the [R-Ladies code of conduct](https://rladies.org/code-of-conduct/).\\\n    In summary, please be nice to each other and help us make an **inclusive** meeting! `<svg aria-hidden=\"true\" role=\"img\" viewBox=\"0 0 512 512\" style=\"height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:purple;overflow:visible;position:relative;\"><path d=\"M244 84L255.1 96L267.1 84.02C300.6 51.37 347 36.51 392.6 44.1C461.5 55.58 512 115.2 512 185.1V190.9C512 232.4 494.8 272.1 464.4 300.4L283.7 469.1C276.2 476.1 266.3 480 256 480C245.7 480 235.8 476.1 228.3 469.1L47.59 300.4C17.23 272.1 0 232.4 0 190.9V185.1C0 115.2 50.52 55.58 119.4 44.1C164.1 36.51 211.4 51.37 244 84C243.1 84 244 84.01 244 84L244 84zM255.1 163.9L210.1 117.1C188.4 96.28 157.6 86.4 127.3 91.44C81.55 99.07 48 138.7 48 185.1V190.9C48 219.1 59.71 246.1 80.34 265.3L256 429.3L431.7 265.3C452.3 246.1 464 219.1 464 190.9V185.1C464 138.7 430.4 99.07 384.7 91.44C354.4 86.4 323.6 96.28 301.9 117.1L255.1 163.9z\"/></svg>`{=html}\n\n## Support Vector Machines\n\n-   Supervised Learning Algorithm\n-   Binary classification by means of separating hyperplane\n-   Can be extended to more than two classes\n-   Can also be used for regression\n\n## Hyperplane\n\n![](screenprints/fig14_1.png)\n\n-   Object one dimension less than dimensions of space\n-   in d dim: $f(X)=\\beta_0 + \\beta_1X_1 + \\ldots+\\beta_pX_p = 0$\n-   $Y \\in \\{-1,1\\}$\n-   $f(X)>0$: point X on one side, $f(X)<0$: point X on other side\n-   $Y_i*f(X_i)>0$, point on correct side of plane\n\n## Hard Margin Classifier\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n![](screenprints/fig14_2B.png)\n:::\n\n::: {.column width=\"50%\" .fragment}\n![](screenprints/fig14_3.png)\n:::\n\n::::\n\n\n-   Boundary with maximum separation (2M) between classes\n-   Maximizing distance to closest points from either class\n-   Points on the margin are Support Vectors\n\n## Soft Margin Classifier {.smaller}\n\n-   Sometimes data are separable by hyperplane\n-   HMC not robust for noisy data\n-   Solution:\n    -   allow points in margin or on wrong side hyperplane\n    -   budget for wrong points $C$ (hyperparameter $\\rightarrow$ tuning)\n\n. . . \n\n![](screenprints/fig14_5.png)\n\n## But then this:  \n\n:::: {.columns}\n\n::: {.column width=\"33%\"}\n![](screenprints/fig14_6a.png)\n:::\n\n::: {.column width=\"33%\" .fragment}\n![](screenprints/fig14_6b.png)\n:::\n\n::: {.column width=\"33%\" .fragment}\n![](screenprints/fig14_6c.png)\n:::\n\n::::\n\n-  Enlarge the feature space by adding more features\n-  Here with $X_3 = X_1^2 + X_2^2$, a polynomial function degree 2\n-  With new feature space hyperplane still linear, not so in original feature space\n\n## Support Vector Machines\n\n-  Enlarge feature space in structured way, with *kernels*\n-  Polynomial kernel degree $d$ and scale $\\gamma$:\n     -  $K(x_i,x_{i'}) = \\gamma(1+\\sum_{j=1}^p x_{ij}x_{i'j})^d$\n-  Radial kernel: \n    -  $K(x_i,x_{i'}) = exp( \\gamma \\sum_{j=1}^p (x_{ij}-x_{i'j})^2)$, with $\\gamma = \\frac 1 {2\\sigma^2}$\n-  ...\n-  Hyperparameters found with tuning\n-  SVMs are extremely flexible and capable of estimating complex nonlinear decision boundaries\n-  Advice authors: start with radial kernel\n\n## Example  1/n\n\n:::: {.columns}\n\n::: {.column width=\"52%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Libraries needed\nlibrary(tidyverse)\nlibrary(kernlab)  # fitting SVMs\nlibrary(mlbench)  # ML benchmark data sets\n\n# Simulate data\nset.seed(0841)\nspirals <- as.data.frame(\n  mlbench.spirals(300,\n                  cycles = 2,\n                  sd = 0.09))\nnames(spirals) <- c(\"x1\", \"x2\", \"classes\")\nhead(spirals)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         x1          x2 classes\n1 0.3894633 -0.01786672       1\n2 0.2731469  0.04359156       1\n3 0.3394452  0.05940869       1\n4 0.1959808  0.09491952       1\n5 0.2001946 -0.58237355       2\n6 0.3331377  0.12047611       1\n```\n:::\n:::\n\n\n\n\n:::\n\n::: {.column width=\"47%\" .fragment}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# make a plot\nggplot(spirals, aes(x = x1, y = x2)) +\n  geom_point(aes(shape = classes,\n                 color = classes),\n             size = 3, alpha = 0.75) +\n  xlab(expression(X[1])) +\n  ylab(expression(X[2])) +\n  xlim(-2, 2) +\n  ylim(-2, 2) +\n  coord_fixed() +\n  theme_bw(base_size = 20) +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](HOML14SVM_files/figure-revealjs/unnamed-chunk-3-1.png){width=960}\n:::\n:::\n\n\n:::\n\n::::\n\n\n## Example 2/n  \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit an SVM using a radial basis function kernel\nspirals_svm <- ksvm(classes ~ x1 + x2, \n                    data = spirals, \n                    #Radial Basis kernel \"Gaussian\":\n                    kernel = \"rbfdot\",\n                    C = 500, \n                    prob.model = TRUE)\n```\n:::\n\n\n. . . \n\n\n::: {.cell}\n\n```{.r .cell-code}\nspirals_svm\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSupport Vector Machine object of class \"ksvm\" \n\nSV type: C-svc  (classification) \n parameter : cost C = 500 \n\nGaussian Radial Basis kernel function. \n Hyperparameter : sigma =  1.56383735596073 \n\nNumber of Support Vectors : 82 \n\nObjective Function Value : -15568.76 \nTraining error : 0.023333 \nProbability model included. \n```\n:::\n:::\n\n\n\n## Example 3/n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Grid over which to evaluate decision boundaries\nnpts <- 500\nxgrid <- expand.grid(\n  x1 = seq(from = -2, 2, length = npts),\n  x2 = seq(from = -2, 2, length = npts)\n)\n\n# Predicted probabilities (as a two-column matrix)\nprob_svm <- predict(spirals_svm, \n                    newdata = xgrid,\n                    type = \"probabilities\")\n\nxgrid2 <- bind_cols(xgrid, prob = prob_svm[,1])\n```\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(xgrid2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         x1 x2      prob\n1 -2.000000 -2 0.2344511\n2 -1.991984 -2 0.2353928\n3 -1.983968 -2 0.2363789\n4 -1.975952 -2 0.2374113\n5 -1.967936 -2 0.2384918\n6 -1.959920 -2 0.2396225\n```\n:::\n:::\n\n\n\n## Example 4/n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Scatterplots with decision boundaries\nggplot(spirals, aes(x = x1, y = x2)) +\n  geom_point(aes(shape = classes,\n                 color = classes),\n             size = 3, alpha = 0.75) +\n  xlab(expression(X[1])) +\n  ylab(expression(X[2])) +\n  xlim(-2, 2) +\n  ylim(-2, 2) +\n  coord_fixed() +\n  theme_bw(base_size = 20) +\n  theme(legend.position = \"none\") +\n  stat_contour(data = xgrid2, \n               aes(x = x1,\n                   y = x2, \n                   z = prob), \n               linewidth = 1,\n               breaks = 0.5,\n               color = \"black\")\n```\n:::\n\n:::\n\n::: {.column width=\"50%\" .fragment}\n\n::: {.cell}\n::: {.cell-output-display}\n![](HOML14SVM_files/figure-revealjs/contour-1.png){width=480}\n:::\n:::\n\n\n:::\n\n::::\n\n## Extensions\n\n-  **More than two classes**\n    -  One vs all: fit SVM for each class (one class vs the rest), classify to class with largest margin\n    -  One vs one: fit all pairwise svms (1 vs 2, 1 vs 3, ...) and most voted class wins\n-  **Support vector regression**\n    -   find a good fitting hyperplane in a kernel-induced feature space that will have good generalization performance using the original features\n\n\n## Example Job attrition  1/n\n\n-  Intent to predict *Attrition*\n-  Tune and fit an SVM with a radial kernel (hyperparameters \\sigma and C)\n-  K-fold cv, can be time consuming\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load attrition data\ndf <- modeldata::attrition %>% \n  #change all factors to unordered factors\n  mutate_if(is.ordered, factor, ordered = FALSE)\n\n# Create training (70%) and test (30%) sets\nset.seed(123)  # for reproducibility\nlibrary(rsample)\nchurn_split <- initial_split(df, prop = 0.7, strata = \"Attrition\")\nchurn_train <- training(churn_split)\nchurn_test  <- testing(churn_split)\n```\n:::\n\n\n\n## Example Job attrition  2/n\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Tune an SVM with radial basis kernel\nlibrary(caret)\nset.seed(1854)  # for reproducibility\nchurn_svm <- caret::train(\n  Attrition ~ ., \n  data = churn_train,\n  method = \"svmRadial\",               \n  preProcess = c(\"center\", \"scale\"),  \n  trControl = trainControl(method = \"cv\",\n                           number = 10),\n  tuneLength = 10\n)\n\n# different results than in book?\nggplot(churn_svm) + \n  geom_text(aes(label = C),\n            hjust = \"inward\",\n            nudge_y = -0.001) + \n  theme_light()\n```\n:::\n\n:::\n\n::: {.column width=\"50%\" .fragment}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](HOML14SVM_files/figure-revealjs/jobbplot-1.png){width=480}\n:::\n:::\n\n\n\n:::\n\n::::\n\n\n##  Example Job attrition  3/n\n\n-  Probabilities is not naturally for SVM, but can be \"estimated\" \n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Control params for SVM\nctrl <- trainControl(method = \"cv\",  number = 10, classProbs = TRUE, \n                     summaryFunction = twoClassSummary ) # needed for AUC/ROC\n# Tune an SVM\nset.seed(5628)  # for reproducibility\nchurn_svm_auc <- train(Attrition ~ ., \n                       data = churn_train, method = \"svmRadial\", \n                       preProcess = c(\"center\", \"scale\"), \n                       metric = \"ROC\",  # area under ROC curve (AUC)\n                       trControl = ctrl, tuneLength = 10)\n\nchurn_svm_auc$results %>% round(4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    sigma      C    ROC   Sens   Spec  ROCSD SensSD SpecSD\n1  0.0094   0.25 0.8238 0.9641 0.3688 0.0589 0.0149 0.0838\n2  0.0094   0.50 0.8240 0.9652 0.3816 0.0588 0.0173 0.0689\n3  0.0094   1.00 0.8243 0.9652 0.3757 0.0586 0.0197 0.0946\n4  0.0094   2.00 0.8271 0.9791 0.3504 0.0584 0.0092 0.1022\n5  0.0094   4.00 0.8234 0.9826 0.3022 0.0583 0.0113 0.0826\n6  0.0094   8.00 0.8122 0.9837 0.3129 0.0543 0.0098 0.1370\n7  0.0094  16.00 0.7957 0.9837 0.2827 0.0532 0.0098 0.1288\n8  0.0094  32.00 0.7864 0.9826 0.3018 0.0537 0.0147 0.1380\n9  0.0094  64.00 0.7865 0.9861 0.2710 0.0540 0.0107 0.1104\n10 0.0094 128.00 0.7865 0.9837 0.2776 0.0540 0.0098 0.1209\n```\n:::\n:::\n\n\n\n## Feature Interpretation  \n\n-  SVMs do not emit any natural measures of feature importance\n-  We can use `vip()`\n\n. . .\n\n:::: {.columns}\n\n::: {.column width=\"65%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# We want reference class \"Yes\" \n# Make function returning prob of \"Yes\" \nprob_yes <- function(object, newdata) {\n  predict(object, newdata = newdata, type = \"prob\")[, \"Yes\"]\n}\n\nset.seed(2827)  # for reproducibility\nvip::vip(churn_svm_auc, \n         method = \"permute\",\n         nsim = 5, \n         train = churn_train, \n         target = \"Attrition\", \n         metric = \"auc\", \n         reference_class = \"Yes\", \n         pred_wrapper = prob_yes) +\n  theme_bw(base_size = 12)\n```\n:::\n\n\n:::\n\n::: {.column width=\"35%\" .fragment}\n\n::: {.cell}\n::: {.cell-output-display}\n![](HOML14SVM_files/figure-revealjs/vipplot-1.png){width=480}\n:::\n:::\n\n\n:::\n\n::::\n\n\n\n## PDP  \n\n\n::: {.cell}\n\n```{.r .cell-code}\nppdp <- function(x){pdp::partial(churn_svm_auc, pred.var = x, which.class = 2,\n                                 prob = TRUE, plot = TRUE, plot.engine = \"ggplot2\") +\n  coord_flip() + theme_bw(base_size = 12)\n}\n#library(patchwork)\nppdp(\"OverTime\") +  ppdp(\"WorkLifeBalance\") + ppdp(\"JobSatisfaction\") + \n  ppdp(\"JobRole\") + plot_layout(ncol = 2)\n```\n:::\n\n\n. . .\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](HOML14SVM_files/figure-revealjs/pdpplot-1.png){width=960}\n:::\n:::\n\n\n\n# The end",
    "supporting": [
      "HOML14SVM_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\r\n<script>\r\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\r\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\r\n  // slide changes (different for each slide format).\r\n  (function () {\r\n    function fireSlideChanged(previousSlide, currentSlide) {\r\n\r\n      // dispatch for htmlwidgets\r\n      const event = window.document.createEvent(\"Event\");\r\n      event.initEvent(\"slideenter\", true, true);\r\n      window.document.dispatchEvent(event);\r\n\r\n      // dispatch for shiny\r\n      if (window.jQuery) {\r\n        if (previousSlide) {\r\n          window.jQuery(previousSlide).trigger(\"hidden\");\r\n        }\r\n        if (currentSlide) {\r\n          window.jQuery(currentSlide).trigger(\"shown\");\r\n        }\r\n      }\r\n    }\r\n\r\n    // hookup for reveal\r\n    if (window.Reveal) {\r\n      window.Reveal.addEventListener(\"slidechanged\", function(event) {\r\n        fireSlideChanged(event.previousSlide, event.currentSlide);\r\n      });\r\n    }\r\n\r\n    // hookup for slidy\r\n    if (window.w3c_slidy) {\r\n      window.w3c_slidy.add_observer(function (slide_num) {\r\n        // slide_num starts at position 1\r\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\r\n      });\r\n    }\r\n\r\n  })();\r\n</script>\r\n\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}